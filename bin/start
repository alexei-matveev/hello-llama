#!/bin/bash -x

if [ ! -d models/ ]; then
    echo 'No models/ here!'
    exit 1
fi

# Generational Models:
#
#   19M     stories15M-q4_0.gguf
#   1.5G    ggml-model-q4_0.gguf
#   2.3G    Phi-3-mini-4k-instruct-q4.gguf
#   4.1G    mistral-7b-v0.1.Q4_K_M.gguf
#   7.4G    amethyst-13b-mistral.Q4_K_M.gguf
#
# Embedding Models:
#   21M    all-MiniLM-L6-v2-Q4_K_M.gguf

# This  embedding  Model  makes  sense only  for  embedding,  no  chat
# completion. The server  will terminate if you ask it  to.  Isnt it a
# BUG? Why not just report an error. Hm, are we doing it wrong?
# url=https://huggingface.co/second-state/All-MiniLM-L6-v2-Embedding-GGUF/resolve/main/all-MiniLM-L6-v2-Q4_K_M.gguf

# url=https://huggingface.co/ggml-org/models/resolve/main/tinyllamas/stories15M-q4_0.gguf
# url=https://huggingface.co/ggml-org/models/resolve/main/phi-2/ggml-model-q4_0.gguf

# -chat-template=phi3
url=https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-q4.gguf

# url=https://huggingface.co/TheBloke/Mistral-7B-v0.1-GGUF/resolve/main/mistral-7b-v0.1.Q4_K_M.gguf

# --chat-template=llama3 "Instruct" is missing:
# url=https://huggingface.co/QuantFactory/Meta-Llama-3-8B-GGUF/resolve/main/Meta-Llama-3-8B.Q4_K_M.gguf

# unknown model architecture: 'gemma2'
# --chat-template=gemma?
# url=https://huggingface.co/bartowski/gemma-2-9b-it-GGUF/resolve/main/gemma-2-9b-it-Q4_K_M.gguf

# This will need ~9G RAM and will be really slow on CPU. Maybe think
# of grammars like root ::= "yes" | "no" ..
# url=https://huggingface.co/TheBloke/Amethyst-13B-Mistral-GGUF/resolve/main/amethyst-13b-mistral.Q4_K_M.gguf

model=$(basename $url)

if [ ! -f models/$model ]; then
    echo "Downloading $url ..."
    curl -L -o "models/$model" "$url"
fi

# There seems to be less swapping with --no-mmap, but large models may
# lock you laptop on loading!
#
# With --embeddings  flag we tell  the server to offer  Embedding API.
# There seem to be two endpoints enabled with one flag [1]:
#
#   --embeddings: Enable embedding vector output and the OAI compatible
#       endpoint /v1/embeddings. Physical batch size (--ubatch-size) must
#       be carefully defined. Default: disabled
#
# Example   uses  see   the  description   of  "POST   /v1/embeddings:
# OpenAI-compatible  embeddings API"  in the  Server README  [1]. Note
# that OpenAI Endpoint /v1/embeddings  is plural whereas /embedding is
# singular.   Note use  of  `input`  in OpenAI  API  and `content`  in
# llama.cpp proper. OpenAPI /v1/embeddings  appears to accept an array
# of strings in the `input` field as well [2].
#
#   curl -sX POST http://localhost:8080/embedding \
#	 -d '{"content": "some text to embed"}'
#
#   curl -sX POST http://localhost:8080/v1/embeddings \
#        -H "Authorization: Bearer no-key" \
#        -H "Content-Type: application/json" \
#        -d '{"input": ["hello", "world"],
#             "model": "phi-3-mini-4k-instruct",
#             "encoding_format": "float"}'
#   | jq | less
#
# See also other approaches [3].
#
# [1] https://github.com/ggerganov/llama.cpp/blob/master/examples/server/README.md
# [2] https://platform.openai.com/docs/api-reference/embeddings
# [3] https://neuml.hashnode.dev/rag-with-llamacpp-and-external-api-services

podman run --rm \
       -p 8080:8080 \
       -v $PWD/models:/models \
       localhost/llama-cpp \
       -m models/$model \
       --alias $(basename $model .gguf) \
       --chat-template phi3 \
       --embeddings \
       --no-mmap \
       --log-format text \
       --host 0.0.0.0 \
       --metrics \
       -c 2048 \
       $*
