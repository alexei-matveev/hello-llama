#!/bin/bash -x

if [ ! -d models/ ]; then
    echo 'No models/ here!'
    exit 1
fi

# Model sizes:
#
#   19M     stories15M-q4_0.gguf
#   1.5G    ggml-model-q4_0.gguf
#   2.3G    Phi-3-mini-4k-instruct-q4.gguf
#   4.1G    mistral-7b-v0.1.Q4_K_M.gguf
#   7.4G    amethyst-13b-mistral.Q4_K_M.gguf


#rl=https://huggingface.co/ggml-org/models/resolve/main/tinyllamas/stories15M-q4_0.gguf
#rl=https://huggingface.co/ggml-org/models/resolve/main/phi-2/ggml-model-q4_0.gguf

# -chat-template=phi3
url=https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-q4.gguf
#rl=https://huggingface.co/bartowski/Phi-3-medium-4k-instruct-GGUF/resolve/main/Phi-3-medium-4k-instruct-Q4_K_M.gguf

#rl=https://huggingface.co/TheBloke/Mistral-7B-v0.1-GGUF/resolve/main/mistral-7b-v0.1.Q4_K_M.gguf

# --chat-template=llama3 "Instruct" is missing:
#rl=https://huggingface.co/QuantFactory/Meta-Llama-3-8B-GGUF/resolve/main/Meta-Llama-3-8B.Q4_K_M.gguf

# This will need ~9G RAM and will be really slow on CPU. Maybe think
# of grammars like root ::= "yes" | "no" ..
#rl=https://huggingface.co/TheBloke/Amethyst-13B-Mistral-GGUF/resolve/main/amethyst-13b-mistral.Q4_K_M.gguf

model=$(basename $url)

if [ ! -f models/$model ]; then
    echo "Downloading $url ..."
    curl -L -o "models/$model" "$url"
fi

# There seems to be less swapping with --no-mmap, but large models may
# lock you laptop on loading!
#
# With --embeddings  flag we tell  the server to offer  Embedding API.
# There seem to be two endpoints enabled with one flag [1]:
#
#   --embeddings: Enable embedding vector output and the OAI compatible
#       endpoint /v1/embeddings. Physical batch size (--ubatch-size) must
#       be carefully defined. Default: disabled
#
# Example   uses  see   the  description   of  "POST   /v1/embeddings:
# OpenAI-compatible  embeddings API"  in the  Server README  [1]. Note
# that OpenAI Endpoint /v1/embeddings  is plural whereas /embedding is
# singular. Note use of `input` in OpenAI API and `content`
# OpenAPI /v1/embeddings  appears  to accept  an array  of
# strings in the `input` field [2].
#
#   curl -sX POST http://localhost:8080/embedding \
#	 -d '{"content": "some text to embed"}'
#
#   curl -sX POST http://localhost:8080/v1/embeddings \
#        -H "Authorization: Bearer no-key" \
#        -H "Content-Type: application/json" \
#        -d '{"input": ["hello", "world"],
#             "model": "phi-3-mini-4k-instruct",
#             "encoding_format": "float"}'
#   | jq | less
#
# [1] https://github.com/ggerganov/llama.cpp/blob/master/examples/server/README.md
# [2] https://platform.openai.com/docs/api-reference/embeddings

podman run --rm \
       -p 8080:8080 \
       -v $PWD/models:/models \
       localhost/llama-cpp \
       -m models/$model \
       --chat-template phi3 \
       --embeddings \
       --no-mmap \
       --log-format text \
       --host 0.0.0.0 \
       --metrics \
       -c 2048 \
       $*
