#!/bin/bash -x

if [ ! -d models/ ]; then
    echo 'No models/ here!'
    exit 1
fi

# Model sizes:
#
#   19M     stories15M-q4_0.gguf
#   1.5G    ggml-model-q4_0.gguf
#   2.3G    Phi-3-mini-4k-instruct-q4.gguf
#   4.1G    mistral-7b-v0.1.Q4_K_M.gguf
#   7.4G    amethyst-13b-mistral.Q4_K_M.gguf


#rl=https://huggingface.co/ggml-org/models/resolve/main/tinyllamas/stories15M-q4_0.gguf
#rl=https://huggingface.co/ggml-org/models/resolve/main/phi-2/ggml-model-q4_0.gguf

# -chat-template=phi3
url=https://huggingface.co/microsoft/Phi-3-mini-4k-instruct-gguf/resolve/main/Phi-3-mini-4k-instruct-q4.gguf
#rl=https://huggingface.co/bartowski/Phi-3-medium-4k-instruct-GGUF/resolve/main/Phi-3-medium-4k-instruct-Q4_K_M.gguf

#rl=https://huggingface.co/TheBloke/Mistral-7B-v0.1-GGUF/resolve/main/mistral-7b-v0.1.Q4_K_M.gguf

# --chat-template=llama3 "Instruct" is missing:
#rl=https://huggingface.co/QuantFactory/Meta-Llama-3-8B-GGUF/resolve/main/Meta-Llama-3-8B.Q4_K_M.gguf

# This will need ~9G RAM and will be really slow on CPU. Maybe think
# of grammars like root ::= "yes" | "no" ..
#rl=https://huggingface.co/TheBloke/Amethyst-13B-Mistral-GGUF/resolve/main/amethyst-13b-mistral.Q4_K_M.gguf

model=$(basename $url)

if [ ! -f models/$model ]; then
    echo "Downloading $url ..."
    curl -L -o "models/$model" "$url"
fi

# There seems to be less swapping with --no-mmap, but large models may
# lock you laptop on loading!
#
# With --embeddings  flag we tell  the server to offer  Embedding API.
# There seem to be two endpoints enabled with one flag [1]:
#
#   --embeddings: Enable embedding vector output and the OAI compatible
#       endpoint /v1/embeddings. Physical batch size (--ubatch-size) must
#       be carefully defined. Default: disabled
#
# Example uses, note /v1/embeddings is plural whereas /embedding is
# singular:
#
#   curl -sX POST "http://localhost:8080/embedding"     -d '{"content": "some text to embed"}'
#   curl -sX POST "http://localhost:8080/v1/embeddings" -d '{"content": "some text to embed"}'
#
#   curl -sX POST https://localhost:8080/embedding \
#     -H "Authorization: Bearer $AUTH_TOKEN" \
#     -H "Content-Type: application/json" \
#     -d '{"model": "phi-3-mini-4k-instruct",
#          "content": "some text to embed",
#          "encoding_format": "float"}'
#     | jq .
#
# [1] https://github.com/ggerganov/llama.cpp/blob/master/examples/server/README.md

podman run --rm \
       -p 8080:8080 \
       -v $PWD/models:/models \
       localhost/llama-cpp \
       -m models/$model \
       --chat-template phi3 \
       --embeddings \
       --no-mmap \
       --log-format text \
       --host 0.0.0.0 \
       --metrics \
       -c 2048 \
       $*
