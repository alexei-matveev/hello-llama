#!/usr/bin/env python
"""This example session works mostly as intended with
Phi-3-mini-4k-instruct-q4.gguf model.  It starts to break apart when
switching to other languages.  Even German is not always perfect.
Sometimes the model translates all of the chat history to german
instead of replying to the one question it was asked.  One probably
schould not mix languages in one session.
"""

# import os
from json import dumps
from openai import OpenAI

# Phi-3 uses tokens <|end|> to signal the end of the completion that
# appear to leak through the API. Also whitespace in front of the
# completion is irritating in regular case. Also seen "<|assistant|>"
# token in the middle of the completion as if the models decided to
# give another, usually longer, reply. Is it a failure mode?
def trim_content (text: str) -> str:
    return text.strip().removesuffix("<|end|>")

def session (client: OpenAI, prompt: str, questions: list[str]) -> list[dict]:
    messages = [{ "role": "system", "content": prompt}]
    print (prompt)
    for q in questions:
        print ("Q: ", q)
        messages = messages + [ {"role": "user", "content": q}]
        completion = client.chat.completions.create(
            messages=messages,
            # Probably ignored:
            model="gpt-3.5-turbo")

        # Phi-3 replies with "<|end|>" tokens that are not stripped
        # off on parsing of completions. Is it a problem if we pass
        # the token back unchanged? It might be, as the models seems
        # to occasionally engage in discussion with itself.  Also we
        # schould not be putting dict[] and isomorfic
        # ChatCompletionMessage into the same list here.
        messages = messages + \
            [{"role": c.message.role,
              "content": trim_content(c.message.content)}
             for c in completion.choices]

        # print (completion.to_json())
        for c in completion.choices:
            print ("A: ", trim_content(c.message.content))

    # Or should we return List of ChatCompletions?
    return messages

def main ():
    # Default api_key is to take it from OPENAI_API_KEY environment
    # variable:
    client = OpenAI(
        base_url="http://localhost:8080/v1",
        api_key="usually-not-required",
    )

    prompt = """
    Reply with a short sentence or just a few words. No notes!
   """
    # Bitte die Antworten kurz fassen!
    # Будь кратким!

    questions = [
        "Who is James Bond?",
        "Is he dangerous?",
        "Who wrote it all?",
        "When was he born?",
        "Is he alive?",
        # "Wer ist James Bond?",
        # "Кто такой Джеймс Бонд?",
        # "Were did we stop?"
    ]

    # Run this particular session
    # transcript = session (client, prompt, questions)
    #print (dumps(transcript, indent=4))

    text = """
    The main goal of llama.cpp is to enable LLM inference
    with minimal setup and state-of-the-art performance on a wide
    variety of hardware - locally and in the cloud.

    - Plain C/C++ implementation without any dependencies
    - Apple silicon is a first-class citizen - optimized via ARM NEON,
      Accelerate and Metal frameworks
    - AVX, AVX2 and AVX512 support for x86 architectures
    - 1.5-bit, 2-bit, 3-bit, 4-bit, 5-bit, 6-bit, and 8-bit integer
      quantization for faster inference and reduced memory use
    - Custom CUDA kernels for running LLMs on NVIDIA GPUs (support for
      AMD GPUs via HIP)
    - Vulkan and SYCL backend support
    - CPU+GPU hybrid inference to partially accelerate models larger
      than the total VRAM capacity

    Since its inception, the project has improved significantly
    thanks to many contributions. It is the main playground for
    developing new features for the ggml library.
    """

    transcript = session (
        client,
        "Please summarize text. Use a few short and simple sentences.",
        [text])


main()
