#!/usr/bin/env jsonnet
// -*- mode: jsonnet -*-

// Usage:
//
//     $ ./template -m .
//     $ kubectl create namespace hello-llama
//     $ kubectl apply -k . --dry-run=server
//     $ kubectl apply -k .
//

local values = {
  image: 'localhost/llama-cpp:latest',

  // In Jsonnet >= 0.19 there is std.splitLimitR(). Was it worth it?
  model: std.reverse(std.split(self.modelUrl, '/'))[0],
  modelUrl: 'https://huggingface.co/microsoft/phi-3-mini-4k-instruct-gguf/resolve/main/phi-3-mini-4k-instruct-q4.gguf',
  modelAlias: std.strReplace(self.model, '.gguf', ''),

  // We assume here that *.localhost resolves to the local machine.
  // Following URL should be directed to corresponding Service, also
  // with https:// protocoll. Example URLs:
  //
  //     $ curl -s http://phi-3-mini-4k-instruct-q4.localhost/v1/models | jq
  //
  host: self.modelAlias + '.localhost',
};

{
  'kustomization.yaml':
    {
      namespace: 'hello-llama',
      namePrefix: '',
      nameSuffix: '',
      resources: [
        'deployment.json',
      ],
    },

  // Kind = List in a single file:
  'deployment.json':
    {
      apiVersion: 'v1',
      kind: 'List',
      items: [
        {
          apiVersion: 'apps/v1',
          kind: 'Deployment',
          metadata: {
            name: 'llama-server',
          },
          spec: {
            replicas: 1,
            selector: {
              matchLabels: {
                component: 'llama-server',
              },
            },
            template: {
              metadata: {
                labels: {
                  component: 'llama-server',
                },
              },
              spec: {
                volumes: [
                  {
                    name: 'llama-models',
                    persistentVolumeClaim: {
                      claimName: 'llama-models',
                    },
                  },
                ],

                // Ugly hack with string templates to get the model
                // file, if missing. In k3s permissions appear to
                // suffice to write to /models volume. This is likely
                // to change in OCP. Should we checksum the file
                // instead of checkin for existence?
                initContainers: [
                  {
                    name: 'init-model',
                    image: values.image,
                    imagePullPolicy: 'IfNotPresent',
                    command: [
                      'sh',
                      '-c',
                      'cd /models && [ -f %s ] || curl -LO %s' % [values.model, values.modelUrl],
                    ],
                    volumeMounts: [
                      { mountPath: '/models', name: 'llama-models' },
                    ],
                  },
                ],

                // The command line ist not generic, see context size
                // and caht template that actually depend on the
                // model.
                containers: [
                  {
                    name: 'llama-server',
                    image: values.image,
                    imagePullPolicy: 'IfNotPresent',
                    // command: ["tail", "-f", "/dev/null"]
                    args: [
                      '--model',
                      '/models/' + values.model,
                      '--alias',
                      values.modelAlias,
                      '--chat-template',
                      'phi3',  // parametrize?
                      '--embeddings',
                      '--no-mmap',
                      '--host',
                      '0.0.0.0',
                      '--metrics',
                      '--ctx-size',
                      '4096',  // parametrize?
                    ],
                    ports: [
                      { containerPort: 8080 },
                    ],
                    volumeMounts: [
                      { mountPath: '/models', name: 'llama-models' },
                    ],
                    env: [
                      { name: 'XXX', value: 'yyy' },
                    ],
                  },
                ],
              },
            },
          },
        },

        {
          apiVersion: 'v1',
          kind: 'Service',
          metadata: {
            name: 'llama-server',
          },
          spec: {
            type: 'ClusterIP',
            selector: {
              component: 'llama-server',
            },
            ports: [
              {
                port: 80,
                targetPort: 8080,
              },
            ],
          },
        },

        // Kind = Ingress:
        {
          apiVersion: 'networking.k8s.io/v1',
          kind: 'Ingress',
          metadata: {
            name: 'llama-server',
          },
          spec: {
            rules: [
              {
                host: values.host,
                http: {
                  paths: [
                    {
                      path: '/',
                      pathType: 'Prefix',
                      backend: {
                        service: {
                          name: 'llama-server',
                          port: {
                            number: 80,
                          },
                        },
                      },
                    },
                  ],
                },
              },
            ],
          },
        },

        {
          // https://github.com/rancher/local-path-provisioner
          apiVersion: 'v1',
          kind: 'PersistentVolumeClaim',
          metadata: {
            name: 'llama-models',
          },
          spec: {
            accessModes: [
              'ReadWriteOnce',
              // When you  dont specify  storage class at  all the  default storage
              // class may be chosen. In k3s with local-path provisioner it is also
              // the  default one.  Omitted  storageClassName is  not  the same  as
              // storageClassName = "".
              // storageClassName: local-path
            ],
            resources: {
              requests: {
                storage: '4Gi',
              },
            },
          },
        },

      ],
    },

}
