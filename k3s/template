#!/usr/bin/env jsonnet
// -*- mode: jsonnet -*-

// Usage:
//
//     $ ./template -m .
//     $ kubectl create namespace hello-llama
//     $ kubectl apply -k . --dry-run=server
//     $ kubectl apply -k .
//
{
  'kustomization.yaml':
    {
      namespace: 'hello-llama',
      namePrefix: '',
      nameSuffix: '',
      resources: [
        'deployment.json',
      ],
    },

  // Kind = List in a single file:
  'deployment.json':
    {
      apiVersion: 'v1',
      kind: 'List',
      items: [
        {
          apiVersion: 'apps/v1',
          kind: 'Deployment',
          metadata: {
            name: 'llama-server',
          },
          spec: {
            replicas: 1,
            selector: {
              matchLabels: {
                component: 'llama-server',
              },
            },
            template: {
              metadata: {
                labels: {
                  component: 'llama-server',
                },
              },
              spec: {
                volumes: [
                  {
                    name: 'llama-models',
                    persistentVolumeClaim: {
                      claimName: 'llama-models',
                    },
                  },
                ],
                containers: [
                  {
                    name: 'llama-server',
                    image: 'localhost/llama-cpp:latest',
                    imagePullPolicy: 'IfNotPresent',
                    // command: ["tail"]
                    // args: ["-f", "/dev/null"]
                    args: [
                      '--model',
                      // "/models/stories15M-q4_0.gguf"
                      '/models/Phi-3-mini-4k-instruct-q4.gguf',
                      '--chat-template',
                      'phi3',
                      '--embeddings',
                      '--no-mmap',
                      '--host',
                      '0.0.0.0',
                      '--metrics',
                      '--ctx-size',
                      '2048',
                    ],
                    ports: [
                      {
                        containerPort: 8080,
                      },
                    ],
                    volumeMounts: [
                      {
                        mountPath: '/models',
                        name: 'llama-models',
                      },
                    ],
                    env: [
                      {
                        name: 'XXX',
                        value: 'yyy',
                      },
                    ],
                  },
                ],
              },
            },
          },
        },

        {
          apiVersion: 'v1',
          kind: 'Service',
          metadata: {
            name: 'llama-server',
          },
          spec: {
            type: 'ClusterIP',
            selector: {
              component: 'llama-server',
            },
            ports: [
              {
                port: 80,
                targetPort: 8080,
              },
            ],
          },
        },

        {
          //
          // We  assume here  that  *.localhost resolves  to  the local  machine.
          // Following URL should be directed to corresponding Service, also with
          // https:// protocoll:
          //
          // http://llama.localhost
          //
          apiVersion: 'networking.k8s.io/v1',
          kind: 'Ingress',
          metadata: {
            name: 'llama-server',
          },
          spec: {
            rules: [
              {
                host: 'llama-server.localhost',
                http: {
                  paths: [
                    {
                      path: '/',
                      pathType: 'Prefix',
                      backend: {
                        service: {
                          name: 'llama-server',
                          port: {
                            number: 80,
                          },
                        },
                      },
                    },
                  ],
                },
              },
            ],
          },
        },

        {
          // https://github.com/rancher/local-path-provisioner
          apiVersion: 'v1',
          kind: 'PersistentVolumeClaim',
          metadata: {
            name: 'llama-models',
          },
          spec: {
            accessModes: [
              'ReadWriteOnce',
              // When you  dont specify  storage class at  all the  default storage
              // class may be chosen. In k3s with local-path provisioner it is also
              // the  default one.  Omitted  storageClassName is  not  the same  as
              // storageClassName = "".
              // storageClassName: local-path
            ],
            resources: {
              requests: {
                storage: '4Gi',
              },
            },
          },
        },

      ],
    },

}
