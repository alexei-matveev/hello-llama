#!/usr/bin/env jsonnet
// -*- mode: jsonnet -*-

// Usage:
//
//     $ ./template -m .
//     $ kubectl create namespace hello-llama
//     $ kubectl apply -k . --dry-run=server
//     $ kubectl apply -k .
//

local values = {
  image: 'localhost/llama-cpp:b3278',

  modelUrl: 'https://huggingface.co/microsoft/phi-3-mini-4k-instruct-gguf/resolve/main/phi-3-mini-4k-instruct-q4.gguf',

  // In Jsonnet >= 0.19 there is std.splitLimitR(). Was it worth it?
  modelFile: std.reverse(std.split(self.modelUrl, '/'))[0],
  modelAlias: std.strReplace(self.modelFile, '.gguf', ''),

  // We assume here that *.localhost resolves to the local machine.
  // Following URL should be directed to corresponding Service, also
  // with https:// protocoll. Example URLs:
  //
  //     $ curl -s http://phi-3-mini-4k-instruct-q4.localhost/v1/models | jq
  //
  host: self.modelAlias + '.localhost',
};


local llamaObjects = [
  {
    kind: 'Deployment',
    apiVersion: 'apps/v1',
    metadata: {
      name: 'llama-server',
    },
    spec: {
      replicas: 1,
      selector: {
        matchLabels: {
          component: 'llama-server',
        },
      },
      template: {
        metadata: {
          labels: {
            component: 'llama-server',
          },
        },
        spec: {
          volumes: [
            {
              name: 'llama-models',
              persistentVolumeClaim: {
                claimName: 'llama-models',
              },
            },
          ],

          // Ugly hack with string templates to get the model
          // file, if missing. In k3s permissions appear to
          // suffice to write to /models volume. This is likely
          // to change in OCP. Should we checksum the file
          // instead of checkin for existence?
          initContainers: [
            {
              name: 'init-model',
              image: values.image,
              imagePullPolicy: 'IfNotPresent',
              command: [
                'sh',
                '-c',
                'cd /models && [ -f %s ] || curl -LO %s' % [values.modelFile, values.modelUrl],
              ],
              volumeMounts: [
                { mountPath: '/models', name: 'llama-models' },
              ],
            },
          ],

          // The command line ist not generic, see context size
          // and caht template that actually depend on the
          // model.
          containers: [
            {
              name: 'llama-server',
              image: values.image,
              imagePullPolicy: 'IfNotPresent',
              // command: ["tail", "-f", "/dev/null"]
              args: [
                '--model',
                '/models/' + values.modelFile,
                '--alias',
                values.modelAlias,
                '--chat-template',
                'phi3',  // parametrize?
                '--embeddings',
                '--no-mmap',
                '--host',
                '0.0.0.0',
                '--metrics',
                '--ctx-size',
                '4096',  // parametrize?
              ],
              ports: [
                { containerPort: 8080 },
              ],
              volumeMounts: [
                { mountPath: '/models', name: 'llama-models' },
              ],
              env: [
                { name: 'XXX', value: 'yyy' },
              ],
            },
          ],
        },
      },
    },
  },

  {
    apiVersion: 'v1',
    kind: 'Service',
    metadata: {
      name: 'llama-server',
    },
    spec: {
      type: 'ClusterIP',
      selector: {
        component: 'llama-server',
      },
      ports: [
        {
          port: 80,
          targetPort: 8080,
        },
      ],
    },
  },

  // Kind = Ingress:
  {
    apiVersion: 'networking.k8s.io/v1',
    kind: 'Ingress',
    metadata: {
      name: 'llama-server',
    },
    spec: {
      rules: [
        {
          host: values.host,
          http: {
            paths: [
              {
                path: '/',
                pathType: 'Prefix',
                backend: {
                  service: {
                    name: 'llama-server',
                    port: {
                      number: 80,
                    },
                  },
                },
              },
            ],
          },
        },
      ],
    },
  },

  {
    // https://github.com/rancher/local-path-provisioner
    apiVersion: 'v1',
    kind: 'PersistentVolumeClaim',
    metadata: {
      name: 'llama-models',
    },
    spec: {
      accessModes: [
        'ReadWriteOnce',
        // When you  dont specify  storage class at  all the  default storage
        // class may be chosen. In k3s with local-path provisioner it is also
        // the  default one.  Omitted  storageClassName is  not  the same  as
        // storageClassName = "".
        // storageClassName: local-path
      ],
      resources: {
        requests: {
          storage: '4Gi',
        },
      },
    },
  },

];

// Open WebUI as a frontend to OpenAI API of Llama Server.  See
// documentation [1] and maybe the discussion [2]
//
// Navigate to Open WebUI, http://open-webui.localhost, and configure
// OpenAI Endpoints, e.g. `http://llama-server/v1` with arbitrary API
// key. See also `OPENAI_API_BASE_URLS` env var below. FWIW, it does
// not look like the system prompt from the GUI has any effect on the
// model replies.
//
// [1] https://docs.openwebui.com
// [2] https://github.com/ggerganov/llama.cpp/discussions/7712
local openWebUiObjects = [
  {
    kind: 'Deployment',
    apiVersion: 'apps/v1',
    metadata: {
      name: 'open-webui',
    },
    spec: {
      replicas: 1,
      selector: {
        matchLabels: {
          component: 'open-webui',
        },
      },
      template: {
        metadata: {
          labels: {
            component: 'open-webui',
          },
        },
        spec: {
          volumes: [
            {
              name: 'open-webui-data',
              persistentVolumeClaim: {
                claimName: 'open-webui-data',
              },
            },
          ],

          containers: [
            {
              name: 'open-webui',
              // Image is 3.6G as of 2024-06-30:
              image: 'ghcr.io/open-webui/open-webui:main',
              imagePullPolicy: 'IfNotPresent',
              ports: [
                { containerPort: 8080 },
              ],
              volumeMounts: [
                { mountPath: '/app/backend/data', name: 'open-webui-data' },
              ],

              // There appears to be more than one API Endpoint
              // possible. Both environment variables accept semicolon
              // separated lists [1].
              //
              // [1] https://docs.openwebui.com/tutorial/openai
              env: [
                { name: 'OPENAI_API_BASE_URLS', value: 'http://llama-server/v1' },
                { name: 'OPENAI_API_KEYS', value: 'secret-key' },
              ],
            },
          ],
        },
      },
    },
  },

  {
    kind: 'Service',
    apiVersion: 'v1',
    metadata: {
      name: 'open-webui',
    },
    spec: {
      type: 'ClusterIP',
      selector: {
        component: 'open-webui',
      },
      ports: [
        { port: 80, targetPort: 8080 },
      ],
    },
  },

  {
    kind: 'Ingress',
    apiVersion: 'networking.k8s.io/v1',
    metadata: {
      name: 'open-webui',
    },
    spec: {
      rules: [
        {
          host: 'open-webui.localhost',
          http: {
            paths: [
              {
                path: '/',
                pathType: 'Prefix',
                backend: {
                  service: {
                    name: 'open-webui',
                    port: { number: 80 },
                  },
                },
              },
            ],
          },
        },
      ],
    },
  },

  {
    kind: 'PersistentVolumeClaim',
    apiVersion: 'v1',
    metadata: {
      name: 'open-webui-data',
    },
    spec: {
      accessModes: ['ReadWriteOnce'],
      resources: {
        requests: { storage: '1Gi' },
      },
    },
  },

];


// Template output with filenames as keys:
{
  'kustomization.yaml':
    {
      namespace: 'hello-llama',
      namePrefix: '',
      nameSuffix: '',
      resources: [
        'deployment.json',
      ],
    },

  // Kind = List in a single file:
  'deployment.json':
    {
      apiVersion: 'v1',
      kind: 'List',
      items: llamaObjects + openWebUiObjects,
    },

}
